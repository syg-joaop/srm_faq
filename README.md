# SRM FAQ - Sistema de FAQ Vetorizado com IA

Sistema completo de FAQ com busca semÃ¢ntica vetorizada e respostas humanizadas por IA local.

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Admin Panel   â”‚â”€â”€â”€â”€â–¶â”‚   NestJS API    â”‚â”€â”€â”€â”€â–¶â”‚   PostgreSQL    â”‚
â”‚     (HTML)      â”‚     â”‚                 â”‚     â”‚   + pgvector    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚     Ollama      â”‚
                        â”‚  (Embeddings +  â”‚
                        â”‚     Chat)       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Deploy com Dokploy

### 1. Preparar o servidor

```bash
# Clonar/enviar arquivos para o servidor
cd /opt
git clone <seu-repositorio> srm-faq
# ou
scp -r ./srm-faq user@seu-servidor:/opt/
```

### 2. Configurar variÃ¡veis de ambiente

```bash
cd /opt/srm-faq
cp .env.example .env
nano .env  # Editar com suas configuraÃ§Ãµes
```

### 3. Subir os containers

```bash
cd /opt/srm-faq/docker
docker-compose up -d
```

### 4. Baixar os modelos de IA (primeira vez)

```bash
# Aguardar o Ollama iniciar (~30s)
docker exec -it srm-ollama ollama pull nomic-embed-text
docker exec -it srm-ollama ollama pull llama3.2:3b
```

### 5. Verificar se estÃ¡ funcionando

```bash
# Health check
curl http://localhost:3000/health

# Testar Ollama
curl http://localhost:11434/api/tags
```

## ğŸ“ Estrutura do Projeto

```
srm-faq/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml   # OrquestraÃ§Ã£o dos containers
â”‚   â”œâ”€â”€ Dockerfile           # Build do NestJS
â”‚   â””â”€â”€ init.sql             # Script de inicializaÃ§Ã£o do banco
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.ts              # Entrada da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ app.module.ts        # MÃ³dulo principal
â”‚   â”œâ”€â”€ health.controller.ts # Health check
â”‚   â”œâ”€â”€ config/              # ConfiguraÃ§Ãµes
â”‚   â””â”€â”€ faq/                 # MÃ³dulo de FAQ
â”‚       â”œâ”€â”€ faq.controller.ts
â”‚       â”œâ”€â”€ faq.service.ts
â”‚       â”œâ”€â”€ database.service.ts
â”‚       â”œâ”€â”€ ollama.service.ts
â”‚       â””â”€â”€ faq.dto.ts
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html           # Redirect para admin
â”‚   â””â”€â”€ admin/
â”‚       â””â”€â”€ index.html       # Painel administrativo
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ .env.example
```

## ğŸ”Œ API Endpoints

### FAQs (CRUD)

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| GET | `/api/faq` | Listar todas FAQs |
| GET | `/api/faq/:id` | Buscar FAQ por ID |
| POST | `/api/faq` | Criar nova FAQ |
| PUT | `/api/faq/:id` | Atualizar FAQ |
| DELETE | `/api/faq/:id` | Excluir FAQ |

### Busca e Chat

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| POST | `/api/faq/search` | Busca semÃ¢ntica |
| POST | `/api/faq/chat` | Chat com IA |

### UtilitÃ¡rios

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| GET | `/api/faq/stats` | EstatÃ­sticas |
| GET | `/api/faq/categories` | Listar categorias |
| GET | `/health` | Health check |

## ğŸ“ Exemplos de Uso

### Criar FAQ

```bash
curl -X POST http://localhost:3000/api/faq \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Como redefinir minha senha?",
    "answer": "Acesse a tela de login e clique em Esqueci minha senha...",
    "category": "Conta",
    "tags": ["senha", "login"]
  }'
```

### Busca SemÃ¢ntica

```bash
curl -X POST http://localhost:3000/api/faq/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "esqueci minha senha",
    "threshold": 0.5,
    "limit": 5
  }'
```

### Chat com IA

```bash
curl -X POST http://localhost:3000/api/faq/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Como faÃ§o para trocar minha senha?",
    "sessionId": "opcional-para-contexto"
  }'
```

## ğŸ¯ IntegraÃ§Ã£o com seu Sistema SRM

### Widget de Chat (Frontend)

```html
<script>
const SRM_FAQ_API = 'https://faq.seudominio.com/api/faq';

async function askFAQ(question) {
  const response = await fetch(`${SRM_FAQ_API}/chat`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ 
      message: question,
      sessionId: localStorage.getItem('faqSessionId')
    })
  });
  
  const result = await response.json();
  localStorage.setItem('faqSessionId', result.data.sessionId);
  return result.data.answer;
}
</script>
```

### Backend (NestJS/Node)

```typescript
import { HttpService } from '@nestjs/axios';

@Injectable()
export class FaqIntegrationService {
  constructor(private http: HttpService) {}

  async chat(message: string, sessionId?: string) {
    const { data } = await this.http.post('http://srm-faq-api:3000/api/faq/chat', {
      message,
      sessionId
    }).toPromise();
    
    return data;
  }
}
```

## ğŸ”§ ConfiguraÃ§Ã£o de ProduÃ§Ã£o

### Nginx Reverse Proxy

```nginx
server {
    listen 80;
    server_name faq.seudominio.com;

    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

### Requisitos de Hardware

| Componente | MÃ­nimo | Recomendado |
|------------|--------|-------------|
| CPU | 2 cores | 4+ cores |
| RAM | 4 GB | 8+ GB |
| Disco | 20 GB | 50+ GB |
| GPU | - | NVIDIA (opcional) |

### Modelos de IA por Capacidade

| RAM DisponÃ­vel | Modelo Recomendado |
|----------------|-------------------|
| 4 GB | llama3.2:1b |
| 6-8 GB | llama3.2:3b |
| 8-16 GB | llama3.1:8b |
| 16+ GB | llama3.1:70b |

## ğŸ› Troubleshooting

### Ollama nÃ£o inicia
```bash
docker logs srm-ollama
# Se GPU NVIDIA, verificar drivers:
nvidia-smi
```

### Embeddings lentos
```bash
# Verificar se modelo estÃ¡ carregado
docker exec srm-ollama ollama list
```

### Banco nÃ£o conecta
```bash
docker logs srm-postgres
docker exec -it srm-postgres psql -U srm_user -d srm_faq -c "SELECT 1"
```

## ğŸ“„ LicenÃ§a

MIT
